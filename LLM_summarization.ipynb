{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10e6591a",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Requirement - Demonstrate the Simple text summarization using LLMs hosted in local environment </h2>\n",
    "<br>\n",
    "<pre>\n",
    "    <b>1. Priority - Data security</b>\n",
    "        a. To run the llms in local environment <br>\n",
    "    <b>2. Resource constraint - Only GPU(H100) of mem size:16GB is available</b>\n",
    "        a. To use quantized llm's which occupies 5GB of GPU Mem and goes upto ~7 to 8GB during inference<br>\n",
    "    <b>3. Assumptions - Simple text passage of 2000 words maximum ( ~6000 tokens)</b>\n",
    "        a. Just keep it simple - Ignoring the complexities of different chain types\n",
    "       \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634199f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import ('pysqlite3')\n",
    "#import sys\n",
    "#sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n",
    "\n",
    "#----threads & basics\n",
    "import os\n",
    "from threading import Thread\n",
    "import torch\n",
    "\n",
    "#----Gradio front-end\n",
    "#from IPython.display import Image, display, HTML\n",
    "#import gradio as gr\n",
    "\n",
    "#----llms\n",
    "from transformers import AutoTokenizer, pipeline,TextIteratorStreamer \n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "#----embeddings & vectordb\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da661c1",
   "metadata": {},
   "source": [
    "\n",
    "<pre><b>               Defining LLM and its parameters ; Loading of LLM (Mistral 7b instruct) to GPU</b></pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb29596-6994-4987-84bd-01a7156dccf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_input = \" \"\n",
    "model_id = 'mistralai/Mistral-7B-Instruct-v0.2' #----LLM used\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "#----streams the words generated by llms, instead of waiting for the entire output to return. \n",
    "streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3899e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'mistralai/Mistral-7B-Instruct-v0.2' #----LLM used\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True) \n",
    "\n",
    "llm = pipeline(\n",
    "                \"text-generation\",\n",
    "                model = model_id, \n",
    "                tokenizer = tokenizer, #----Mistral's tokenizer\n",
    "                device_map = 'auto',#----Loads the LLM on multiple GPU, if multiple GPU exists\n",
    "                temperature = '0.0', #----avoids hallucinations\n",
    "                max_length = 8000, #----max no.of words in in the summary\n",
    "                repetition_penalty=1.3,#----Which prevents the same words getting repeated \n",
    "                                       #----llama-2 & mistral suffers from generating endless whiteline\n",
    "                                       #----Repetition penalty is used to handle them.\n",
    "                streamer = streamer,\n",
    "                model_kwargs={\"torch_dtype\": torch.bfloat16, \"load_in_4bit\": True} \n",
    "                                       #----Once the model is loaded, weights' converted/quantized to 4bits\n",
    "                                       #----Thus helps in reduced GPU mem usage\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c57782e",
   "metadata": {},
   "source": [
    "\n",
    "<pre><b>\n",
    "                             1. Input texts to chunks;\n",
    "                             2. Chunks to vector embeddings\n",
    "                             </b></pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a10f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def defining_vector_store():\n",
    "    global global_input\n",
    "    #----Input texts to chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(separators=['\\n'], chunk_size=3000, chunk_overlap=300) \n",
    "    input_tokens = global_input.strip()\n",
    "    docs = text_splitter.create_documents([input_tokens])\n",
    "    \n",
    "    for i, x in enumerate(docs): #----inserting page number for each chunks (expectation of langchain libs)\n",
    "        x.metadata[\"source\"] = f\"{i}-pl\"\n",
    "\n",
    "    embedding = HuggingFaceEmbeddings()\n",
    "    vectordb = Chroma.from_documents(documents=docs, embedding = embedding) #----Converts chunks to vector embeddings\n",
    "    return vectordb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24de0ac7",
   "metadata": {},
   "source": [
    "\n",
    "<pre><b>\n",
    "Once user enters the text passages in GUI,summarize function will be invoked, it performs the following actions\n",
    "                       1. defines the Prompt for summarization wrt Mistral model\n",
    "                       2. Calls LLM thread to handle the output stream\n",
    "\n",
    "</b></pre>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730b064d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(user_input): \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    global global_input\n",
    "    global_input = user_input\n",
    "    user_prompt = \"Write a concise and a short summary of the following text.\"  #-----Defining prompts for summarization\n",
    "    template = \"\"\"\n",
    "                <s>[INST] {user_prompt} [/INST]\n",
    "                Text: `{text}`\n",
    "                Answer:\n",
    "                </s>  \n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(input_variables =['text', 'user_prompt'], template-template) \n",
    "    prompt_format = prompt.format(text = summarize_input, user_prompt= user_prompt )\n",
    "\n",
    "    local_11ms = HuggingFacePipeline(pipeline=11m) \n",
    "    t= Thread(target=local_11ms, args = (prompt_format,)).start() #----Thread to handle the LLM output stream\n",
    "    out=\" \"\n",
    "    for new_text in streamer: #----words are sent to GUI as soon as its generated; instead of waiting for entire output\n",
    "        out += new_text\n",
    "        yield out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf18e81",
   "metadata": {},
   "source": [
    "\n",
    "<pre><b>                           defining Front-end (or) GUI        </b></pre>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a520b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks () as demo:\n",
    "    with gr.Tab(\"Summarization\"):\n",
    "        gr.Markdown(\n",
    "        \"<center><h1>Text Summarization </h1>\\n</center>\\n\"\n",
    "        )\n",
    "        with gr.Column():\n",
    "            with gr.Row():\n",
    "                user_input = gr.Textbox(label=\"User input\", lines=10, scale=2, placeholder=\"Enter your texts...\") \n",
    "                model_output = gr.Textbox(label=\"Model output\", lines=10, scale =2, interactive=False)\n",
    "                \n",
    "            submit = gr.Button(value=\"Submit\")\n",
    "        submit.click(summarize, inputs = user_input, outputs = model_output)\n",
    "\n",
    "    demo.queue (max_size=16).launch(share=True, server_port = int('5000')) #----it launches the link, where GUI is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86e15520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"./topical_chat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d64eee6-2eba-4b5e-bb77-c535fa13827c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Are you a fan of Google or Microsoft?'\n",
      " 'Both are excellent technology they are helpful in many ways. For the security purpose both are super.'\n",
      " \" I'm not  a huge fan of Google, but I use it a lot because I have to. I think they are a monopoly in some sense. \"\n",
      " ' Google provides online related services and products, which includes online ads, search engine and cloud computing.'\n",
      " \" Yeah, their services are good. I'm just not a fan of intrusive they can be on our personal lives. \"\n",
      " 'Google is leading the alphabet subsidiary and will continue to be the Umbrella company for Alphabet internet interest.'\n",
      " 'Did you know Google had hundreds of live goats to cut the grass in the past?'\n",
      " ' It is very interesting. Google provide \"Chrome OS\" which is a light weight OS. Google provided a lot of hardware mainly in 2010 to 2015. '\n",
      " 'I like Google Chrome. Do you use it as well for your browser?'\n",
      " ' Yes.Google is the biggest search engine and Google service figure out top 100 website, including Youtube and Blogger.'\n",
      " ' By the way, do you like Fish? '\n",
      " 'Yes. They form a sister group of tourniquets- they make the sea water clean and remove the dust from it. Fish is the biggest part in the eco-system.'\n",
      " 'Did you know that a seahorse is the only fish to have a neck?'\n",
      " ' Freshwater fish only drink water through the skin via Osmosis, Saltwater fish drink water through the mouth. Dolphins are friendly to human beings.'\n",
      " ' Interesting, they also have gills. Did you know that jellyfish are immortal? '\n",
      " 'Yes. Fish is the important resources of human world wide for the commercial and subsistence fish hunts the fish in the wild fisheries.'\n",
      " \" What about cats, do you like cats? I'm a dog fan myself. \"\n",
      " 'The cat is referred as domestic cat and wild cat. They make our world very clean from rats!'\n",
      " ' Yeah, cats can be cool, but they sure do spend a lot of their time sleeping. '\n",
      " 'Cats hear the sounds too faint or too high frequency human ears can hear.'\n",
      " ' I heard that too. Well, it was nice chatting with you. Have a good day. ']\n"
     ]
    }
   ],
   "source": [
    "grp = data.groupby('conversation_id')\n",
    "\n",
    "for i in range (1,101):\n",
    "    print(grp.get_group(i)['message'].values)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bfaabc-c9f9-4edd-a8fc-16b8712f5eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import VLLM\n",
    "\n",
    "llm = VLLM(\n",
    "    model=\"TheBloke/Llama-2-7b-Chat-AWQ\",\n",
    "    trust_remote_code=True,\n",
    "    max_new_tokens=128,\n",
    "    vllm_kwargs={\"quantization\": \"awq\"},\n",
    ")\n",
    "\n",
    "print(llm.invoke(\"What is the capital of France ?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700ad739-3112-4aad-9935-f1ae6a5d7e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
